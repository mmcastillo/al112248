{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8PDt847jiAWZ3jLpO1gl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmcastillo/al112248/blob/main/modelo1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he1raG8TN9jT",
        "outputId": "a9870d40-0605-4fc1-b7a8-1cbbf44145a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (5.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install natsort"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/');\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.layers import Activation, MaxPool2D, Concatenate\n",
        "from scipy import ndimage\n",
        "import random\n",
        "import natsort\n",
        "import glob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRIWbDUTQFTZ",
        "outputId": "53a039f1-075a-4da0-f1c2-038715318349"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_dir = '/content/drive/MyDrive/DOCTORADO/python/datos_para_entrenar_modelos/modelo_1.X/train/50p_rand_50n/imgs/';\n",
        "train_mask_dir = '/content/drive/MyDrive/DOCTORADO/python/datos_para_entrenar_modelos/modelo_1.X/train/50p_rand_50n/masks/';\n",
        "\n",
        "train_img_list_temp = os.listdir(train_img_dir)\n",
        "#train_img_list = natsort.natsorted(train_img_list)\n",
        "train_mask_list_temp = os.listdir(train_mask_dir)\n",
        "#train_mask_list = natsort.natsorted(train_mask_list)\n",
        "\n",
        "#Se aleatoriza la lista del conjunto de entrenamiento\n",
        "train_img_list=[]\n",
        "train_mask_list=[]\n",
        "inx = random.sample(range(len(train_img_list_temp)), len(train_img_list_temp))  \n",
        "for i in range(len(inx)):\n",
        "  train_img_list.append(train_img_list_temp[inx[i]])\n",
        "  train_mask_list.append(train_mask_list_temp[inx[i]])\n",
        "\n",
        "val_img_dir = '/content/drive/MyDrive/DOCTORADO/python/datos_para_entrenar_modelos/modelo_1.X/val/imgs/';\n",
        "val_mask_dir = '/content/drive/MyDrive/DOCTORADO/python/datos_para_entrenar_modelos/modelo_1.X/val/masks/';\n",
        "val_img_list = os.listdir(val_img_dir)\n",
        "val_img_list = natsort.natsorted(val_img_list)\n",
        "val_mask_list = os.listdir(val_mask_dir)\n",
        "val_mask_list = natsort.natsorted(val_mask_list)"
      ],
      "metadata": {
        "id": "MruxLoBYQqFh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_img(img_dir, img_list):\n",
        "    images=[]\n",
        "    for i, image_name in enumerate(img_list):    \n",
        "        if (image_name.split('.')[1] == 'npy'):\n",
        "            \n",
        "            image = np.load(img_dir+image_name)\n",
        "                      \n",
        "            images.append(image)\n",
        "    images = np.array(images)\n",
        "    \n",
        "    return(images)"
      ],
      "metadata": {
        "id": "bTAaIf_5QPqC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U8qqs4WJwYwE"
      },
      "outputs": [],
      "source": [
        "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n",
        "\n",
        "    L = len(img_list)\n",
        "\n",
        "    #keras needs the generator infinite, so we will use while true  \n",
        "    while True:\n",
        "\n",
        "        batch_start = 0\n",
        "        batch_end = batch_size\n",
        "\n",
        "        while batch_start < L:\n",
        "            limit = min(batch_end, L)\n",
        "                       \n",
        "            X = load_img(img_dir, img_list[batch_start:limit])\n",
        "            Y = load_img(mask_dir, mask_list[batch_start:limit])\n",
        "\n",
        "            X = X.astype(np.float32)  \n",
        "            Y = Y.astype(np.float32)\n",
        "\n",
        "            #I = img_list[batch_start:limit]\n",
        "            #M = mask_list[batch_start:limit]\n",
        "\n",
        "\n",
        "            yield (X,Y)#,I,M) #a tuple with two numpy arrays with batch_size samples\n",
        "           \n",
        "            batch_start += batch_size   \n",
        "            batch_end += batch_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=4\n",
        "\n",
        "train_img_datagen = imageLoader(train_img_dir, train_img_list, \n",
        "                                train_mask_dir, train_mask_list,batch_size)\n",
        "\n",
        "val_img_datagen = imageLoader(val_img_dir, val_img_list, \n",
        "                                val_mask_dir, val_mask_list,batch_size)\n"
      ],
      "metadata": {
        "id": "gwccjU68Qr7L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img,mask = train_img_datagen.__next__()\n",
        "print(img.shape)\n",
        "print(mask.shape)\n",
        "print(np.unique(mask))\n",
        "\n",
        "#Encuentra los patches 3D donde las máscaras contienen valores 1 y 0 (donde hay aneurisams)\n",
        "#if np.unique(mask).shape[0]  == 2:\n",
        "  #for i in range(mask.shape[0]):\n",
        "    #if np.unique(mask[i,:,:,:]).shape[0] == 2: \n",
        "      #n_patch_aneurisma = i\n",
        "      #print(f'En el patch número {n_patch_aneurisma+1} se encuentra el aneurisma')\n",
        "      #n_imagenes_aneurismas=[]\n",
        "      #for j in range(mask.shape[1]):\n",
        "        #if np.unique(mask[n_patch_aneurisma,j,:,:]).shape[0] == 2:\n",
        "          #print(f'En las imagenes {j+1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KHHIyA0zdAl",
        "outputId": "feb936e7-51ad-4328-87e1-883c430aca07"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 64, 64, 64, 1)\n",
            "(4, 64, 64, 64, 1)\n",
            "[0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bv3JM39CV_tU"
      },
      "outputs": [],
      "source": [
        "def conv_block(input, num_filters):\n",
        "    x = Conv3D(num_filters, 3, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)   #Not in the original network. \n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv3D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)  #Not in the original network\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#Encoder block: Conv block followed by maxpooling\n",
        "\n",
        "\n",
        "def encoder_block(input, num_filters):\n",
        "    x = conv_block(input, num_filters)\n",
        "    p = MaxPooling3D((2, 2, 2))(x)\n",
        "    return x, p   \n",
        "\n",
        "#Decoder block\n",
        "#skip features gets input from encoder for concatenation\n",
        "\n",
        "def decoder_block(input, skip_features, num_filters):\n",
        "    x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=\"same\")(input)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "#Build Unet using the blocks\n",
        "def build_unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 16)\n",
        "    s2, p2 = encoder_block(p1, 32)\n",
        "    s3, p3 = encoder_block(p2, 64)\n",
        "    s4, p4 = encoder_block(p3, 128)\n",
        "\n",
        "    b1 = conv_block(p4, 256) #Bridge\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 128)\n",
        "    d2 = decoder_block(d1, s3, 64)\n",
        "    d3 = decoder_block(d2, s2, 32)\n",
        "    d4 = decoder_block(d3, s1, 16)\n",
        "\n",
        "    activation = 'sigmoid'\n",
        "\n",
        "    outputs = Conv3D(1, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
        "    print(f'activation function: {activation}')\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6qkE9MIgBYB",
        "outputId": "ae63fc37-0b96-4031-a834-21e2754e60ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activation function: sigmoid\n",
            "model input shape: (None, 64, 64, 64, 1)\n"
          ]
        }
      ],
      "source": [
        "model = build_unet((64, 64, 64, 1))\n",
        "print(f'model input shape: {model.input_shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ant9LozBclBY",
        "outputId": "3deb29fe-feab-4ab5-f35f-476cc169a879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation-models-3D\n",
            "  Downloading segmentation_models_3D-1.0.4-py3-none-any.whl (33 kB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting classification-models-3D>=1.0.6\n",
            "  Downloading classification_models_3D-1.0.6-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-3D) (2.9.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->segmentation-models-3D) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->segmentation-models-3D) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (1.50.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (21.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (1.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (14.0.6)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (0.27.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->segmentation-models-3D) (4.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->segmentation-models-3D) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->segmentation-models-3D) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->segmentation-models-3D) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow>=2.8.0->segmentation-models-3D) (3.0.9)\n",
            "Installing collected packages: keras-applications, classification-models-3D, segmentation-models-3D\n",
            "Successfully installed classification-models-3D-1.0.6 keras-applications-1.0.8 segmentation-models-3D-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4-ffnnX2lDT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a43802-e75c-4eb3-9eec-cf6ac7175e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Models: using `tf.keras` framework.\n"
          ]
        }
      ],
      "source": [
        "from segmentation_models_3D.losses import BinaryCELoss\n",
        "import segmentation_models_3D as sm\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
        "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
        "'''\n",
        "dice_loss = sm.losses.DiceLoss()\n",
        "focal_loss = sm.losses.BinaryFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
        "\n",
        "iou = tf.keras.metrics.MeanIoU(num_classes=2)\n",
        "\n",
        "metrics = [iou, sm.metrics.FScore(), sm.metrics.Precision(), sm.metrics.Recall()]\n",
        "#jaccard_loss = sm.losses.JaccardLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg8cd8WeixXu",
        "outputId": "006f3c27-60c7-444d-960d-24d20ca7c40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"U-Net\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 64, 64, 64,  0           []                               \n",
            "                                 1)]                                                              \n",
            "                                                                                                  \n",
            " conv3d (Conv3D)                (None, 64, 64, 64,   448         ['input_1[0][0]']                \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 64, 64, 64,   64         ['conv3d[0][0]']                 \n",
            " alization)                     16)                                                               \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 64, 64, 64,   0           ['batch_normalization[0][0]']    \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " conv3d_1 (Conv3D)              (None, 64, 64, 64,   6928        ['activation[0][0]']             \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 64, 64, 64,   64         ['conv3d_1[0][0]']               \n",
            " rmalization)                   16)                                                               \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 64, 64, 64,   0           ['batch_normalization_1[0][0]']  \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " max_pooling3d (MaxPooling3D)   (None, 32, 32, 32,   0           ['activation_1[0][0]']           \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " conv3d_2 (Conv3D)              (None, 32, 32, 32,   13856       ['max_pooling3d[0][0]']          \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 32,   128        ['conv3d_2[0][0]']               \n",
            " rmalization)                   32)                                                               \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 32,   0           ['batch_normalization_2[0][0]']  \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv3d_3 (Conv3D)              (None, 32, 32, 32,   27680       ['activation_2[0][0]']           \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 32,   128        ['conv3d_3[0][0]']               \n",
            " rmalization)                   32)                                                               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 32,   0           ['batch_normalization_3[0][0]']  \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " max_pooling3d_1 (MaxPooling3D)  (None, 16, 16, 16,   0          ['activation_3[0][0]']           \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv3d_4 (Conv3D)              (None, 16, 16, 16,   55360       ['max_pooling3d_1[0][0]']        \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 16, 16, 16,   256        ['conv3d_4[0][0]']               \n",
            " rmalization)                   64)                                                               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 16, 16, 16,   0           ['batch_normalization_4[0][0]']  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv3d_5 (Conv3D)              (None, 16, 16, 16,   110656      ['activation_4[0][0]']           \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 16, 16, 16,   256        ['conv3d_5[0][0]']               \n",
            " rmalization)                   64)                                                               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 16, 16, 16,   0           ['batch_normalization_5[0][0]']  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " max_pooling3d_2 (MaxPooling3D)  (None, 8, 8, 8, 64)  0          ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv3d_6 (Conv3D)              (None, 8, 8, 8, 128  221312      ['max_pooling3d_2[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 8, 8, 8, 128  512        ['conv3d_6[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 8, 8, 8, 128  0           ['batch_normalization_6[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv3d_7 (Conv3D)              (None, 8, 8, 8, 128  442496      ['activation_6[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 8, 8, 8, 128  512        ['conv3d_7[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 8, 8, 8, 128  0           ['batch_normalization_7[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling3d_3 (MaxPooling3D)  (None, 4, 4, 4, 128  0          ['activation_7[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv3d_8 (Conv3D)              (None, 4, 4, 4, 256  884992      ['max_pooling3d_3[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 4, 4, 4, 256  1024       ['conv3d_8[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 4, 4, 4, 256  0           ['batch_normalization_8[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv3d_9 (Conv3D)              (None, 4, 4, 4, 256  1769728     ['activation_8[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 4, 4, 4, 256  1024       ['conv3d_9[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 4, 4, 4, 256  0           ['batch_normalization_9[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv3d_transpose (Conv3DTransp  (None, 8, 8, 8, 128  262272     ['activation_9[0][0]']           \n",
            " ose)                           )                                                                 \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 8, 8, 8, 256  0           ['conv3d_transpose[0][0]',       \n",
            "                                )                                 'activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " conv3d_10 (Conv3D)             (None, 8, 8, 8, 128  884864      ['concatenate[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 8, 8, 8, 128  512        ['conv3d_10[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 8, 8, 8, 128  0           ['batch_normalization_10[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv3d_11 (Conv3D)             (None, 8, 8, 8, 128  442496      ['activation_10[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 8, 8, 8, 128  512        ['conv3d_11[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 8, 8, 8, 128  0           ['batch_normalization_11[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv3d_transpose_1 (Conv3DTran  (None, 16, 16, 16,   65600      ['activation_11[0][0]']          \n",
            " spose)                         64)                                                               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 16, 16, 16,   0           ['conv3d_transpose_1[0][0]',     \n",
            "                                128)                              'activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv3d_12 (Conv3D)             (None, 16, 16, 16,   221248      ['concatenate_1[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 16,   256        ['conv3d_12[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 16,   0           ['batch_normalization_12[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv3d_13 (Conv3D)             (None, 16, 16, 16,   110656      ['activation_12[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 16,   256        ['conv3d_13[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 16,   0           ['batch_normalization_13[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv3d_transpose_2 (Conv3DTran  (None, 32, 32, 32,   16416      ['activation_13[0][0]']          \n",
            " spose)                         32)                                                               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 32, 32, 32,   0           ['conv3d_transpose_2[0][0]',     \n",
            "                                64)                               'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv3d_14 (Conv3D)             (None, 32, 32, 32,   55328       ['concatenate_2[0][0]']          \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 32, 32, 32,   128        ['conv3d_14[0][0]']              \n",
            " ormalization)                  32)                                                               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 32, 32, 32,   0           ['batch_normalization_14[0][0]'] \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv3d_15 (Conv3D)             (None, 32, 32, 32,   27680       ['activation_14[0][0]']          \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 32, 32, 32,   128        ['conv3d_15[0][0]']              \n",
            " ormalization)                  32)                                                               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 32, 32, 32,   0           ['batch_normalization_15[0][0]'] \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv3d_transpose_3 (Conv3DTran  (None, 64, 64, 64,   4112       ['activation_15[0][0]']          \n",
            " spose)                         16)                                                               \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 64, 64, 64,   0           ['conv3d_transpose_3[0][0]',     \n",
            "                                32)                               'activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv3d_16 (Conv3D)             (None, 64, 64, 64,   13840       ['concatenate_3[0][0]']          \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 64, 64, 64,   64         ['conv3d_16[0][0]']              \n",
            " ormalization)                  16)                                                               \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 64, 64, 64,   0           ['batch_normalization_16[0][0]'] \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " conv3d_17 (Conv3D)             (None, 64, 64, 64,   6928        ['activation_16[0][0]']          \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 64, 64, 64,   64         ['conv3d_17[0][0]']              \n",
            " ormalization)                  16)                                                               \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 64, 64, 64,   0           ['batch_normalization_17[0][0]'] \n",
            "                                16)                                                               \n",
            "                                                                                                  \n",
            " conv3d_18 (Conv3D)             (None, 64, 64, 64,   17          ['activation_17[0][0]']          \n",
            "                                1)                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,650,801\n",
            "Trainable params: 5,647,857\n",
            "Non-trainable params: 2,944\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "LR = 0.01\n",
            "BinaryCrossentropy\n"
          ]
        }
      ],
      "source": [
        "LR = 0.01\n",
        "optim = keras.optimizers.Adam(LR)\n",
        "\n",
        "loss = 'BinaryCrossentropy'\n",
        "\n",
        "model.compile(optimizer=optim, loss=loss ,metrics = metrics)\n",
        "print(model.summary())\n",
        "print(f'LR = {LR}')\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5n_rIbVlQ5I",
        "outputId": "ac87025e-3f64-44c5-d186-d6bc275105f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "225/225 [==============================] - 205s 830ms/step - loss: 0.0351 - mean_io_u: 0.4993 - f1-score: 0.0117 - precision: 0.0097 - recall: 0.1351 - val_loss: 7.5419e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.3895e-05 - val_precision: 8.5542e-06 - val_recall: 0.9063\n",
            "Epoch 2/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0075 - mean_io_u: 0.4993 - f1-score: 0.0255 - precision: 0.0283 - recall: 0.1242 - val_loss: 0.0119 - val_mean_io_u: 0.5000 - val_f1-score: 3.1608e-05 - val_precision: 1.6085e-05 - val_recall: 0.9073\n",
            "Epoch 3/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0066 - mean_io_u: 0.4993 - f1-score: 0.0378 - precision: 0.0460 - recall: 0.1341 - val_loss: 7.6022e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.7150e-06 - val_precision: 4.8884e-06 - val_recall: 0.9063\n",
            "Epoch 4/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0064 - mean_io_u: 0.4993 - f1-score: 0.0442 - precision: 0.0554 - recall: 0.1393 - val_loss: 0.0875 - val_mean_io_u: 0.5000 - val_f1-score: 5.9893e-05 - val_precision: 3.0031e-05 - val_recall: 0.9170\n",
            "Epoch 5/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0063 - mean_io_u: 0.4993 - f1-score: 0.0480 - precision: 0.0638 - recall: 0.1436 - val_loss: 0.0018 - val_mean_io_u: 0.5000 - val_f1-score: 6.2742e-05 - val_precision: 3.5263e-05 - val_recall: 0.9065\n",
            "Epoch 6/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0059 - mean_io_u: 0.4993 - f1-score: 0.0608 - precision: 0.0805 - recall: 0.1564 - val_loss: 3.5034e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.6465e-05 - val_precision: 5.8310e-05 - val_recall: 0.9063\n",
            "Epoch 7/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0059 - mean_io_u: 0.4993 - f1-score: 0.0616 - precision: 0.0813 - recall: 0.1566 - val_loss: 0.0020 - val_mean_io_u: 0.5000 - val_f1-score: 7.1475e-05 - val_precision: 3.9688e-05 - val_recall: 0.9066\n",
            "Epoch 8/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0056 - mean_io_u: 0.4993 - f1-score: 0.0732 - precision: 0.0968 - recall: 0.1649 - val_loss: 1.1421e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.0615e-06 - val_precision: 3.1099e-05 - val_recall: 0.9063\n",
            "Epoch 9/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0054 - mean_io_u: 0.4993 - f1-score: 0.0834 - precision: 0.1094 - recall: 0.1757 - val_loss: 1.8238e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.9260e-05 - val_precision: 2.9452e-05 - val_recall: 0.9063\n",
            "Epoch 10/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0051 - mean_io_u: 0.4993 - f1-score: 0.0958 - precision: 0.1257 - recall: 0.1859 - val_loss: 1.3883e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.8292e-05 - val_precision: 4.0559e-05 - val_recall: 0.9063\n",
            "Epoch 11/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 0.0051 - mean_io_u: 0.4993 - f1-score: 0.1032 - precision: 0.1295 - recall: 0.1943 - val_loss: 1.2284e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.6002e-05 - val_precision: 4.9450e-05 - val_recall: 0.9063\n",
            "Epoch 12/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0049 - mean_io_u: 0.4993 - f1-score: 0.1175 - precision: 0.1448 - recall: 0.2096 - val_loss: 1.1502e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.3747e-06 - val_precision: 1.1551e-05 - val_recall: 0.9063\n",
            "Epoch 13/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0046 - mean_io_u: 0.4993 - f1-score: 0.1364 - precision: 0.1659 - recall: 0.2281 - val_loss: 1.4214e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.2664e-06 - val_precision: 4.0399e-06 - val_recall: 0.9063\n",
            "Epoch 14/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0044 - mean_io_u: 0.4993 - f1-score: 0.1562 - precision: 0.1823 - recall: 0.2503 - val_loss: 1.1305e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.6888e-06 - val_precision: 1.5401e-05 - val_recall: 0.9063\n",
            "Epoch 15/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0041 - mean_io_u: 0.4993 - f1-score: 0.1750 - precision: 0.1999 - recall: 0.2743 - val_loss: 1.1653e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.7515e-06 - val_precision: 1.2106e-05 - val_recall: 0.9063\n",
            "Epoch 16/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0038 - mean_io_u: 0.4993 - f1-score: 0.2032 - precision: 0.2275 - recall: 0.3020 - val_loss: 1.2647e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.9266e-06 - val_precision: 5.2645e-06 - val_recall: 0.9063\n",
            "Epoch 17/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0037 - mean_io_u: 0.4993 - f1-score: 0.2200 - precision: 0.2461 - recall: 0.3232 - val_loss: 0.0022 - val_mean_io_u: 0.5000 - val_f1-score: 9.4936e-05 - val_precision: 5.2276e-05 - val_recall: 0.9068\n",
            "Epoch 18/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0032 - mean_io_u: 0.4993 - f1-score: 0.2726 - precision: 0.3000 - recall: 0.3732 - val_loss: 0.0018 - val_mean_io_u: 0.5000 - val_f1-score: 9.6734e-05 - val_precision: 5.4469e-05 - val_recall: 0.9067\n",
            "Epoch 19/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0029 - mean_io_u: 0.4993 - f1-score: 0.3187 - precision: 0.3417 - recall: 0.4178 - val_loss: 1.2472e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.5764e-06 - val_precision: 6.9949e-06 - val_recall: 0.9063\n",
            "Epoch 20/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0028 - mean_io_u: 0.4993 - f1-score: 0.3364 - precision: 0.3573 - recall: 0.4377 - val_loss: 0.0043 - val_mean_io_u: 0.5000 - val_f1-score: 6.9426e-05 - val_precision: 3.6468e-05 - val_recall: 0.9070\n",
            "Epoch 21/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0026 - mean_io_u: 0.4993 - f1-score: 0.3649 - precision: 0.3872 - recall: 0.4664 - val_loss: 1.1345e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.6167e-06 - val_precision: 1.6579e-05 - val_recall: 0.9063\n",
            "Epoch 22/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0027 - mean_io_u: 0.4993 - f1-score: 0.3467 - precision: 0.3759 - recall: 0.4465 - val_loss: 1.2234e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.0281e-06 - val_precision: 6.7162e-06 - val_recall: 0.9063\n",
            "Epoch 23/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0023 - mean_io_u: 0.4993 - f1-score: 0.4125 - precision: 0.4277 - recall: 0.5128 - val_loss: 1.1786e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.4513e-06 - val_precision: 9.6640e-06 - val_recall: 0.9063\n",
            "Epoch 24/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0022 - mean_io_u: 0.4993 - f1-score: 0.4230 - precision: 0.4420 - recall: 0.5209 - val_loss: 1.8615e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.3268e-05 - val_precision: 7.6199e-05 - val_recall: 0.9063\n",
            "Epoch 25/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0021 - mean_io_u: 0.4993 - f1-score: 0.4492 - precision: 0.4663 - recall: 0.5438 - val_loss: 0.0195 - val_mean_io_u: 0.5000 - val_f1-score: 1.0860e-04 - val_precision: 5.4929e-05 - val_recall: 0.9110\n",
            "Epoch 26/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0023 - mean_io_u: 0.4993 - f1-score: 0.4234 - precision: 0.4497 - recall: 0.5192 - val_loss: 1.2520e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.8854e-06 - val_precision: 1.2211e-05 - val_recall: 0.9063\n",
            "Epoch 27/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0024 - mean_io_u: 0.4993 - f1-score: 0.3977 - precision: 0.4296 - recall: 0.4982 - val_loss: 4.5404e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.1965e-05 - val_precision: 4.8168e-05 - val_recall: 0.9063\n",
            "Epoch 28/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0019 - mean_io_u: 0.4993 - f1-score: 0.4763 - precision: 0.4900 - recall: 0.5743 - val_loss: 0.0177 - val_mean_io_u: 0.5000 - val_f1-score: 1.1449e-04 - val_precision: 5.8004e-05 - val_recall: 0.9106\n",
            "Epoch 29/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0019 - mean_io_u: 0.4993 - f1-score: 0.4759 - precision: 0.4974 - recall: 0.5761 - val_loss: 0.0111 - val_mean_io_u: 0.5000 - val_f1-score: 6.9989e-05 - val_precision: 3.5686e-05 - val_recall: 0.9081\n",
            "Epoch 30/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0020 - mean_io_u: 0.4993 - f1-score: 0.4680 - precision: 0.4914 - recall: 0.5632 - val_loss: 6.9522e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.8508e-05 - val_precision: 6.5919e-05 - val_recall: 0.9064\n",
            "Epoch 31/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0020 - mean_io_u: 0.4993 - f1-score: 0.4885 - precision: 0.5055 - recall: 0.5896 - val_loss: 7.9412e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.1448e-05 - val_precision: 2.0280e-05 - val_recall: 0.9063\n",
            "Epoch 32/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0019 - mean_io_u: 0.4993 - f1-score: 0.4859 - precision: 0.5092 - recall: 0.5851 - val_loss: 1.3287e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.8047e-06 - val_precision: 1.4266e-05 - val_recall: 0.9063\n",
            "Epoch 33/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0023 - mean_io_u: 0.4993 - f1-score: 0.4126 - precision: 0.4412 - recall: 0.5163 - val_loss: 1.0614e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.4689e-06 - val_precision: 5.3364e-05 - val_recall: 0.9063\n",
            "Epoch 34/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0019 - mean_io_u: 0.4993 - f1-score: 0.4847 - precision: 0.5082 - recall: 0.5802 - val_loss: 0.0631 - val_mean_io_u: 0.5000 - val_f1-score: 1.3005e-04 - val_precision: 6.5301e-05 - val_recall: 0.9217\n",
            "Epoch 35/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0018 - mean_io_u: 0.4993 - f1-score: 0.5100 - precision: 0.5296 - recall: 0.6089 - val_loss: 2.7974e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.3897e-05 - val_precision: 5.5091e-05 - val_recall: 0.9063\n",
            "Epoch 36/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0017 - mean_io_u: 0.4993 - f1-score: 0.5349 - precision: 0.5496 - recall: 0.6295 - val_loss: 9.4623e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.0611e-05 - val_precision: 4.3966e-05 - val_recall: 0.9064\n",
            "Epoch 37/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0016 - mean_io_u: 0.4993 - f1-score: 0.5442 - precision: 0.5643 - recall: 0.6351 - val_loss: 0.0070 - val_mean_io_u: 0.5000 - val_f1-score: 1.0706e-04 - val_precision: 5.5297e-05 - val_recall: 0.9079\n",
            "Epoch 38/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0015 - mean_io_u: 0.4993 - f1-score: 0.5559 - precision: 0.5780 - recall: 0.6487 - val_loss: 0.6083 - val_mean_io_u: 0.5000 - val_f1-score: 5.3006e-05 - val_precision: 2.6526e-05 - val_recall: 0.9368\n",
            "Epoch 39/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0016 - mean_io_u: 0.4993 - f1-score: 0.5488 - precision: 0.5681 - recall: 0.6416 - val_loss: 0.0032 - val_mean_io_u: 0.5000 - val_f1-score: 5.9956e-05 - val_precision: 3.2083e-05 - val_recall: 0.9067\n",
            "Epoch 40/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0015 - mean_io_u: 0.4993 - f1-score: 0.5663 - precision: 0.5845 - recall: 0.6582 - val_loss: 0.0252 - val_mean_io_u: 0.5000 - val_f1-score: 1.4433e-04 - val_precision: 7.2916e-05 - val_recall: 0.9133\n",
            "Epoch 41/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0015 - mean_io_u: 0.4993 - f1-score: 0.5700 - precision: 0.5907 - recall: 0.6637 - val_loss: 0.0106 - val_mean_io_u: 0.5000 - val_f1-score: 1.6744e-04 - val_precision: 8.5518e-05 - val_recall: 0.9102\n",
            "Epoch 42/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0016 - mean_io_u: 0.4993 - f1-score: 0.5400 - precision: 0.5613 - recall: 0.6380 - val_loss: 6.2846e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.3820e-05 - val_precision: 5.1090e-05 - val_recall: 0.9064\n",
            "Epoch 43/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0014 - mean_io_u: 0.4993 - f1-score: 0.5769 - precision: 0.5916 - recall: 0.6723 - val_loss: 0.0043 - val_mean_io_u: 0.5000 - val_f1-score: 1.7556e-04 - val_precision: 9.2378e-05 - val_recall: 0.9080\n",
            "Epoch 44/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0019 - mean_io_u: 0.4993 - f1-score: 0.5225 - precision: 0.5456 - recall: 0.6274 - val_loss: 1.2518e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.8654e-06 - val_precision: 9.6117e-06 - val_recall: 0.9063\n",
            "Epoch 45/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0015 - mean_io_u: 0.4993 - f1-score: 0.5680 - precision: 0.5870 - recall: 0.6641 - val_loss: 1.2285e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.6262e-06 - val_precision: 1.5528e-05 - val_recall: 0.9063\n",
            "Epoch 46/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0015 - mean_io_u: 0.4993 - f1-score: 0.5790 - precision: 0.6046 - recall: 0.6767 - val_loss: 5.5550e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.1606e-05 - val_precision: 1.5692e-05 - val_recall: 0.9063\n",
            "Epoch 47/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0017 - mean_io_u: 0.4993 - f1-score: 0.5324 - precision: 0.5651 - recall: 0.6283 - val_loss: 3.1354e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.9491e-05 - val_precision: 9.3672e-05 - val_recall: 0.9064\n",
            "Epoch 48/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0014 - mean_io_u: 0.4993 - f1-score: 0.5816 - precision: 0.5999 - recall: 0.6822 - val_loss: 1.1732e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.1212e-06 - val_precision: 2.7023e-05 - val_recall: 0.9063\n",
            "Epoch 49/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0014 - mean_io_u: 0.4993 - f1-score: 0.5871 - precision: 0.6083 - recall: 0.6860 - val_loss: 1.0284e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1334e-05 - val_precision: 1.1895e-04 - val_recall: 0.9063\n",
            "Epoch 50/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0013 - mean_io_u: 0.4993 - f1-score: 0.6035 - precision: 0.6241 - recall: 0.6985 - val_loss: 1.1506e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.2264e-05 - val_precision: 7.8843e-05 - val_recall: 0.9063\n",
            "Epoch 51/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0013 - mean_io_u: 0.4993 - f1-score: 0.6067 - precision: 0.6345 - recall: 0.6970 - val_loss: 0.0034 - val_mean_io_u: 0.5000 - val_f1-score: 1.9299e-04 - val_precision: 1.0298e-04 - val_recall: 0.9078\n",
            "Epoch 52/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0013 - mean_io_u: 0.4993 - f1-score: 0.6096 - precision: 0.6241 - recall: 0.7065 - val_loss: 6.2203e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.6500e-04 - val_precision: 3.2173e-04 - val_recall: 0.9071\n",
            "Epoch 53/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 0.0012 - mean_io_u: 0.4993 - f1-score: 0.6263 - precision: 0.6429 - recall: 0.7178 - val_loss: 1.3142e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.9849e-05 - val_precision: 6.3560e-05 - val_recall: 0.9063\n",
            "Epoch 54/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0011 - mean_io_u: 0.4993 - f1-score: 0.6412 - precision: 0.6515 - recall: 0.7383 - val_loss: 1.5134e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1530e-05 - val_precision: 1.6749e-05 - val_recall: 0.9063\n",
            "Epoch 55/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0013 - mean_io_u: 0.4993 - f1-score: 0.6155 - precision: 0.6366 - recall: 0.7169 - val_loss: 1.5988e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.1670e-05 - val_precision: 2.6995e-05 - val_recall: 0.9063\n",
            "Epoch 56/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0012 - mean_io_u: 0.4993 - f1-score: 0.6206 - precision: 0.6443 - recall: 0.7116 - val_loss: 1.1312e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.1822e-06 - val_precision: 6.9489e-05 - val_recall: 0.9063\n",
            "Epoch 57/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0011 - mean_io_u: 0.4993 - f1-score: 0.6389 - precision: 0.6506 - recall: 0.7355 - val_loss: 1.5333e-04 - val_mean_io_u: 0.5000 - val_f1-score: 8.4593e-05 - val_precision: 1.1169e-04 - val_recall: 0.9063\n",
            "Epoch 58/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0011 - mean_io_u: 0.4993 - f1-score: 0.6451 - precision: 0.6578 - recall: 0.7391 - val_loss: 1.6147e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.0026e-05 - val_precision: 5.9664e-05 - val_recall: 0.9063\n",
            "Epoch 59/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0012 - mean_io_u: 0.4993 - f1-score: 0.6233 - precision: 0.6437 - recall: 0.7196 - val_loss: 1.4016e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.4679e-05 - val_precision: 3.5165e-05 - val_recall: 0.9063\n",
            "Epoch 60/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 0.0012 - mean_io_u: 0.4993 - f1-score: 0.6304 - precision: 0.6426 - recall: 0.7281 - val_loss: 1.7979e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.0898e-05 - val_precision: 8.1264e-05 - val_recall: 0.9063\n",
            "Epoch 61/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0011 - mean_io_u: 0.4993 - f1-score: 0.6488 - precision: 0.6626 - recall: 0.7413 - val_loss: 1.1534e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.5376e-05 - val_precision: 2.5624e-04 - val_recall: 0.9063\n",
            "Epoch 62/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0011 - mean_io_u: 0.4993 - f1-score: 0.6435 - precision: 0.6612 - recall: 0.7380 - val_loss: 1.1201e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.9750e-06 - val_precision: 7.1881e-05 - val_recall: 0.9063\n",
            "Epoch 63/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0010 - mean_io_u: 0.4993 - f1-score: 0.6615 - precision: 0.6750 - recall: 0.7551 - val_loss: 1.3844e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.2566e-06 - val_precision: 1.5399e-05 - val_recall: 0.9063\n",
            "Epoch 64/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 9.8850e-04 - mean_io_u: 0.4993 - f1-score: 0.6696 - precision: 0.6818 - recall: 0.7593 - val_loss: 1.5132e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.8019e-05 - val_precision: 6.9438e-05 - val_recall: 0.9063\n",
            "Epoch 65/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0012 - mean_io_u: 0.4993 - f1-score: 0.6431 - precision: 0.6622 - recall: 0.7353 - val_loss: 1.2323e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.5890e-06 - val_precision: 2.0556e-05 - val_recall: 0.9063\n",
            "Epoch 66/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 9.7584e-04 - mean_io_u: 0.4993 - f1-score: 0.6723 - precision: 0.6853 - recall: 0.7605 - val_loss: 1.3930e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.3755e-05 - val_precision: 3.1225e-05 - val_recall: 0.9063\n",
            "Epoch 67/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0010 - mean_io_u: 0.4993 - f1-score: 0.6647 - precision: 0.6786 - recall: 0.7592 - val_loss: 1.7453e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.6413e-05 - val_precision: 7.6222e-05 - val_recall: 0.9063\n",
            "Epoch 68/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 9.8178e-04 - mean_io_u: 0.4993 - f1-score: 0.6746 - precision: 0.6872 - recall: 0.7704 - val_loss: 6.9575e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.6066e-05 - val_precision: 4.4555e-05 - val_recall: 0.9064\n",
            "Epoch 69/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0010 - mean_io_u: 0.4993 - f1-score: 0.6660 - precision: 0.6828 - recall: 0.7615 - val_loss: 1.1478e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9541e-06 - val_precision: 2.6926e-05 - val_recall: 0.9063\n",
            "Epoch 70/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0013 - mean_io_u: 0.4993 - f1-score: 0.6164 - precision: 0.6353 - recall: 0.7135 - val_loss: 1.2552e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.5753e-05 - val_precision: 5.8939e-05 - val_recall: 0.9063\n",
            "Epoch 71/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 9.7060e-04 - mean_io_u: 0.4993 - f1-score: 0.6676 - precision: 0.6806 - recall: 0.7592 - val_loss: 1.2766e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.9918e-06 - val_precision: 3.5153e-05 - val_recall: 0.9063\n",
            "Epoch 72/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 8.5421e-04 - mean_io_u: 0.4993 - f1-score: 0.6979 - precision: 0.7050 - recall: 0.7905 - val_loss: 1.6647e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.2847e-05 - val_precision: 7.1805e-05 - val_recall: 0.9063\n",
            "Epoch 73/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 9.2719e-04 - mean_io_u: 0.4993 - f1-score: 0.6898 - precision: 0.7001 - recall: 0.7812 - val_loss: 1.5243e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.0592e-05 - val_precision: 3.8535e-05 - val_recall: 0.9063\n",
            "Epoch 74/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.7483e-04 - mean_io_u: 0.4993 - f1-score: 0.7011 - precision: 0.7101 - recall: 0.7902 - val_loss: 1.1679e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.5855e-06 - val_precision: 6.7696e-05 - val_recall: 0.9063\n",
            "Epoch 75/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.4253e-04 - mean_io_u: 0.4993 - f1-score: 0.7095 - precision: 0.7152 - recall: 0.8006 - val_loss: 1.0736e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1590e-05 - val_precision: 1.3079e-04 - val_recall: 0.9063\n",
            "Epoch 76/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.6937e-04 - mean_io_u: 0.4993 - f1-score: 0.6969 - precision: 0.7086 - recall: 0.7858 - val_loss: 1.6850e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.8321e-05 - val_precision: 6.7228e-05 - val_recall: 0.9063\n",
            "Epoch 77/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.7249e-04 - mean_io_u: 0.4993 - f1-score: 0.6954 - precision: 0.7056 - recall: 0.7899 - val_loss: 1.5843e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.9968e-05 - val_precision: 7.2812e-05 - val_recall: 0.9063\n",
            "Epoch 78/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.2112e-04 - mean_io_u: 0.4994 - f1-score: 0.7078 - precision: 0.7159 - recall: 0.7986 - val_loss: 1.8811e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.1120e-04 - val_precision: 3.4659e-04 - val_recall: 0.9063\n",
            "Epoch 79/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0012 - mean_io_u: 0.4993 - f1-score: 0.6312 - precision: 0.6585 - recall: 0.7281 - val_loss: 1.5169e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.6928e-05 - val_precision: 2.3527e-05 - val_recall: 0.9063\n",
            "Epoch 80/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 8.7582e-04 - mean_io_u: 0.4993 - f1-score: 0.6903 - precision: 0.6984 - recall: 0.7847 - val_loss: 1.6556e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0755e-04 - val_precision: 1.2215e-04 - val_recall: 0.9063\n",
            "Epoch 81/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.2782e-04 - mean_io_u: 0.4993 - f1-score: 0.7116 - precision: 0.7149 - recall: 0.8049 - val_loss: 1.5923e-04 - val_mean_io_u: 0.5000 - val_f1-score: 8.2882e-05 - val_precision: 9.7125e-05 - val_recall: 0.9063\n",
            "Epoch 82/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.1565e-04 - mean_io_u: 0.4993 - f1-score: 0.7144 - precision: 0.7206 - recall: 0.8046 - val_loss: 1.7846e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.2764e-05 - val_precision: 8.4157e-05 - val_recall: 0.9063\n",
            "Epoch 83/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.9339e-04 - mean_io_u: 0.4993 - f1-score: 0.7216 - precision: 0.7222 - recall: 0.8170 - val_loss: 1.7430e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.2386e-04 - val_precision: 1.4000e-04 - val_recall: 0.9063\n",
            "Epoch 84/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.8605e-04 - mean_io_u: 0.4993 - f1-score: 0.7161 - precision: 0.7216 - recall: 0.8099 - val_loss: 1.6686e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.2111e-05 - val_precision: 1.0583e-04 - val_recall: 0.9063\n",
            "Epoch 85/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.8363e-04 - mean_io_u: 0.4994 - f1-score: 0.7185 - precision: 0.7214 - recall: 0.8136 - val_loss: 1.3418e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.5033e-05 - val_precision: 5.7354e-05 - val_recall: 0.9063\n",
            "Epoch 86/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0013 - mean_io_u: 0.4993 - f1-score: 0.6463 - precision: 0.6711 - recall: 0.7438 - val_loss: 0.0017 - val_mean_io_u: 0.5000 - val_f1-score: 3.7108e-05 - val_precision: 2.1425e-05 - val_recall: 0.9064\n",
            "Epoch 87/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 9.0837e-04 - mean_io_u: 0.4993 - f1-score: 0.6793 - precision: 0.6887 - recall: 0.7740 - val_loss: 1.2088e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.0803e-06 - val_precision: 2.8898e-05 - val_recall: 0.9063\n",
            "Epoch 88/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.5524e-04 - mean_io_u: 0.4993 - f1-score: 0.7247 - precision: 0.7285 - recall: 0.8166 - val_loss: 1.4831e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.1103e-05 - val_precision: 7.1057e-05 - val_recall: 0.9063\n",
            "Epoch 89/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.2051e-04 - mean_io_u: 0.4993 - f1-score: 0.7347 - precision: 0.7373 - recall: 0.8250 - val_loss: 1.7140e-04 - val_mean_io_u: 0.5000 - val_f1-score: 8.3741e-05 - val_precision: 9.5138e-05 - val_recall: 0.9063\n",
            "Epoch 90/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 7.1115e-04 - mean_io_u: 0.4993 - f1-score: 0.7364 - precision: 0.7372 - recall: 0.8295 - val_loss: 1.3220e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1547e-05 - val_precision: 3.7277e-05 - val_recall: 0.9063\n",
            "Epoch 91/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 7.1846e-04 - mean_io_u: 0.4993 - f1-score: 0.7320 - precision: 0.7387 - recall: 0.8200 - val_loss: 1.5206e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.1072e-05 - val_precision: 4.9250e-05 - val_recall: 0.9063\n",
            "Epoch 92/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.1276e-04 - mean_io_u: 0.4994 - f1-score: 0.7375 - precision: 0.7392 - recall: 0.8297 - val_loss: 1.5068e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.8293e-05 - val_precision: 5.7787e-05 - val_recall: 0.9063\n",
            "Epoch 93/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.0971e-04 - mean_io_u: 0.4994 - f1-score: 0.7351 - precision: 0.7402 - recall: 0.8261 - val_loss: 1.6054e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.5161e-05 - val_precision: 8.6520e-05 - val_recall: 0.9063\n",
            "Epoch 94/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.2989e-04 - mean_io_u: 0.4994 - f1-score: 0.7301 - precision: 0.7352 - recall: 0.8227 - val_loss: 1.5542e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0685e-04 - val_precision: 1.2540e-04 - val_recall: 0.9063\n",
            "Epoch 95/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.8789e-04 - mean_io_u: 0.4997 - f1-score: 0.7403 - precision: 0.7451 - recall: 0.8297 - val_loss: 1.7956e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.3627e-04 - val_precision: 2.6388e-04 - val_recall: 0.9063\n",
            "Epoch 96/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.1734e-04 - mean_io_u: 0.4993 - f1-score: 0.7356 - precision: 0.7400 - recall: 0.8277 - val_loss: 1.5467e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1402e-04 - val_precision: 1.3387e-04 - val_recall: 0.9063\n",
            "Epoch 97/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 0.0018 - mean_io_u: 0.4994 - f1-score: 0.5563 - precision: 0.5741 - recall: 0.6645 - val_loss: 0.0769 - val_mean_io_u: 0.5000 - val_f1-score: 1.1332e-04 - val_precision: 5.6864e-05 - val_recall: 0.9220\n",
            "Epoch 98/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0013 - mean_io_u: 0.4993 - f1-score: 0.6263 - precision: 0.6415 - recall: 0.7214 - val_loss: 2.4877e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.6190e-05 - val_precision: 3.0095e-05 - val_recall: 0.9063\n",
            "Epoch 99/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0010 - mean_io_u: 0.4993 - f1-score: 0.6725 - precision: 0.6811 - recall: 0.7740 - val_loss: 0.0022 - val_mean_io_u: 0.5000 - val_f1-score: 4.8023e-06 - val_precision: 2.6628e-06 - val_recall: 0.9063\n",
            "Epoch 100/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.0542e-04 - mean_io_u: 0.4993 - f1-score: 0.7111 - precision: 0.7144 - recall: 0.8045 - val_loss: 1.4612e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.2793e-05 - val_precision: 2.0007e-05 - val_recall: 0.9063\n",
            "Epoch 101/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.0627e-04 - mean_io_u: 0.4993 - f1-score: 0.7372 - precision: 0.7391 - recall: 0.8288 - val_loss: 1.2218e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.2605e-05 - val_precision: 9.6340e-05 - val_recall: 0.9063\n",
            "Epoch 102/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.9551e-04 - mean_io_u: 0.4994 - f1-score: 0.7410 - precision: 0.7432 - recall: 0.8324 - val_loss: 1.5641e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.2530e-05 - val_precision: 5.0903e-05 - val_recall: 0.9063\n",
            "Epoch 103/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.6862e-04 - mean_io_u: 0.4994 - f1-score: 0.7435 - precision: 0.7466 - recall: 0.8363 - val_loss: 1.5283e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9930e-05 - val_precision: 3.8787e-05 - val_recall: 0.9063\n",
            "Epoch 104/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.4301e-04 - mean_io_u: 0.4995 - f1-score: 0.7500 - precision: 0.7509 - recall: 0.8428 - val_loss: 1.7582e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.3689e-04 - val_precision: 1.5477e-04 - val_recall: 0.9063\n",
            "Epoch 105/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.2816e-04 - mean_io_u: 0.4997 - f1-score: 0.7542 - precision: 0.7544 - recall: 0.8467 - val_loss: 1.5151e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.4397e-06 - val_precision: 5.8778e-06 - val_recall: 0.9063\n",
            "Epoch 106/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.9204e-04 - mean_io_u: 0.4999 - f1-score: 0.7427 - precision: 0.7457 - recall: 0.8373 - val_loss: 1.5235e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.9735e-05 - val_precision: 2.7705e-05 - val_recall: 0.9063\n",
            "Epoch 107/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 7.1744e-04 - mean_io_u: 0.4998 - f1-score: 0.7369 - precision: 0.7429 - recall: 0.8298 - val_loss: 1.4352e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.3315e-05 - val_precision: 6.7252e-05 - val_recall: 0.9063\n",
            "Epoch 108/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.8670e-04 - mean_io_u: 0.4997 - f1-score: 0.7399 - precision: 0.7457 - recall: 0.8328 - val_loss: 1.6603e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.2273e-05 - val_precision: 5.9787e-05 - val_recall: 0.9063\n",
            "Epoch 109/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.3687e-04 - mean_io_u: 0.4999 - f1-score: 0.7544 - precision: 0.7567 - recall: 0.8462 - val_loss: 1.5543e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1205e-04 - val_precision: 1.4760e-04 - val_recall: 0.9063\n",
            "Epoch 110/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 6.6383e-04 - mean_io_u: 0.5001 - f1-score: 0.7464 - precision: 0.7511 - recall: 0.8408 - val_loss: 1.8255e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.1345e-04 - val_precision: 2.3885e-04 - val_recall: 0.9063\n",
            "Epoch 111/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 6.7174e-04 - mean_io_u: 0.5002 - f1-score: 0.7435 - precision: 0.7513 - recall: 0.8344 - val_loss: 1.9879e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.5741e-07 - val_precision: 2.0997e-06 - val_recall: 0.9063\n",
            "Epoch 112/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.6787e-04 - mean_io_u: 0.4998 - f1-score: 0.7448 - precision: 0.7477 - recall: 0.8387 - val_loss: 1.6397e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.5337e-07 - val_precision: 8.4916e-07 - val_recall: 0.9063\n",
            "Epoch 113/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.2113e-04 - mean_io_u: 0.5007 - f1-score: 0.7588 - precision: 0.7598 - recall: 0.8519 - val_loss: 1.7444e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.9094e-04 - val_precision: 2.1803e-04 - val_recall: 0.9063\n",
            "Epoch 114/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.2168e-04 - mean_io_u: 0.5005 - f1-score: 0.7555 - precision: 0.7578 - recall: 0.8478 - val_loss: 1.5972e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.1516e-07 - val_precision: 1.4852e-06 - val_recall: 0.9063\n",
            "Epoch 115/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.9624e-04 - mean_io_u: 0.5013 - f1-score: 0.7617 - precision: 0.7652 - recall: 0.8537 - val_loss: 1.6604e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.1864e-07 - val_precision: 7.0959e-07 - val_recall: 0.9063\n",
            "Epoch 116/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.8640e-04 - mean_io_u: 0.5018 - f1-score: 0.7657 - precision: 0.7676 - recall: 0.8563 - val_loss: 1.6710e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.3745e-05 - val_precision: 3.8869e-05 - val_recall: 0.9063\n",
            "Epoch 117/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.0259e-04 - mean_io_u: 0.5013 - f1-score: 0.7598 - precision: 0.7651 - recall: 0.8493 - val_loss: 1.6419e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.9418e-05 - val_precision: 4.1875e-05 - val_recall: 0.9063\n",
            "Epoch 118/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.1457e-04 - mean_io_u: 0.5012 - f1-score: 0.7575 - precision: 0.7621 - recall: 0.8481 - val_loss: 1.6993e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.6894e-04 - val_precision: 1.9329e-04 - val_recall: 0.9063\n",
            "Epoch 119/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.9708e-04 - mean_io_u: 0.5013 - f1-score: 0.7603 - precision: 0.7663 - recall: 0.8501 - val_loss: 1.8047e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.4078e-04 - val_precision: 2.7077e-04 - val_recall: 0.9063\n",
            "Epoch 120/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.0358e-04 - mean_io_u: 0.5009 - f1-score: 0.7581 - precision: 0.7635 - recall: 0.8488 - val_loss: 1.9924e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.3735e-04 - val_precision: 3.7848e-04 - val_recall: 0.9063\n",
            "Epoch 121/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 6.0815e-04 - mean_io_u: 0.5013 - f1-score: 0.7601 - precision: 0.7663 - recall: 0.8500 - val_loss: 1.7046e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.3227e-05 - val_precision: 1.4890e-05 - val_recall: 0.9063\n",
            "Epoch 122/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 8.4776e-04 - mean_io_u: 0.4998 - f1-score: 0.7155 - precision: 0.7281 - recall: 0.8107 - val_loss: 0.0061 - val_mean_io_u: 0.5000 - val_f1-score: 4.1499e-05 - val_precision: 2.1570e-05 - val_recall: 0.9068\n",
            "Epoch 123/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.4210e-04 - mean_io_u: 0.4999 - f1-score: 0.7496 - precision: 0.7573 - recall: 0.8411 - val_loss: 1.9329e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.7007e-04 - val_precision: 3.0366e-04 - val_recall: 0.9063\n",
            "Epoch 124/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.0094e-04 - mean_io_u: 0.5004 - f1-score: 0.7623 - precision: 0.7643 - recall: 0.8562 - val_loss: 1.7528e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.7520e-04 - val_precision: 1.9933e-04 - val_recall: 0.9063\n",
            "Epoch 125/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.9352e-04 - mean_io_u: 0.5004 - f1-score: 0.7626 - precision: 0.7637 - recall: 0.8574 - val_loss: 1.7553e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.4246e-06 - val_precision: 1.5818e-06 - val_recall: 0.9063\n",
            "Epoch 126/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.3690e-04 - mean_io_u: 0.5021 - f1-score: 0.7751 - precision: 0.7775 - recall: 0.8671 - val_loss: 1.6673e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.6841e-05 - val_precision: 4.2089e-05 - val_recall: 0.9063\n",
            "Epoch 127/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 5.2557e-04 - mean_io_u: 0.5032 - f1-score: 0.7796 - precision: 0.7799 - recall: 0.8709 - val_loss: 1.7473e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.6717e-06 - val_precision: 2.9699e-06 - val_recall: 0.9063\n",
            "Epoch 128/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.1437e-04 - mean_io_u: 0.5038 - f1-score: 0.7817 - precision: 0.7843 - recall: 0.8720 - val_loss: 1.7270e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.0191e-05 - val_precision: 2.2671e-05 - val_recall: 0.9063\n",
            "Epoch 129/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 5.2558e-04 - mean_io_u: 0.5040 - f1-score: 0.7809 - precision: 0.7843 - recall: 0.8701 - val_loss: 2.1237e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.5118e-04 - val_precision: 5.0357e-04 - val_recall: 0.9063\n",
            "Epoch 130/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 5.6281e-04 - mean_io_u: 0.5026 - f1-score: 0.7717 - precision: 0.7731 - recall: 0.8637 - val_loss: 1.6383e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.7292e-04 - val_precision: 4.3551e-04 - val_recall: 0.9063\n",
            "Epoch 131/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.6418e-04 - mean_io_u: 0.5015 - f1-score: 0.7695 - precision: 0.7751 - recall: 0.8612 - val_loss: 4.3748e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.5166e-06 - val_precision: 1.3068e-06 - val_recall: 0.9063\n",
            "Epoch 132/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.5037e-04 - mean_io_u: 0.5018 - f1-score: 0.7738 - precision: 0.7742 - recall: 0.8673 - val_loss: 1.7192e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.7767e-05 - val_precision: 8.0473e-05 - val_recall: 0.9063\n",
            "Epoch 133/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 5.5080e-04 - mean_io_u: 0.5017 - f1-score: 0.7761 - precision: 0.7782 - recall: 0.8676 - val_loss: 1.7049e-04 - val_mean_io_u: 0.5000 - val_f1-score: 8.4088e-06 - val_precision: 9.7975e-06 - val_recall: 0.9063\n",
            "Epoch 134/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 5.4975e-04 - mean_io_u: 0.5018 - f1-score: 0.7724 - precision: 0.7741 - recall: 0.8663 - val_loss: 1.8934e-04 - val_mean_io_u: 0.5000 - val_f1-score: 8.0449e-05 - val_precision: 9.0281e-05 - val_recall: 0.9063\n",
            "Epoch 135/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.4440e-04 - mean_io_u: 0.5035 - f1-score: 0.7739 - precision: 0.7781 - recall: 0.8666 - val_loss: 1.5693e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.1618e-05 - val_precision: 7.8132e-05 - val_recall: 0.9063\n",
            "Epoch 136/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.2284e-04 - mean_io_u: 0.5038 - f1-score: 0.7807 - precision: 0.7840 - recall: 0.8708 - val_loss: 2.5987e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1079e-07 - val_precision: 1.2331e-07 - val_recall: 0.9063\n",
            "Epoch 137/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.2969e-04 - mean_io_u: 0.5040 - f1-score: 0.7804 - precision: 0.7803 - recall: 0.8738 - val_loss: 1.8151e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.6574e-05 - val_precision: 3.0774e-05 - val_recall: 0.9063\n",
            "Epoch 138/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.6965e-04 - mean_io_u: 0.5041 - f1-score: 0.7737 - precision: 0.7805 - recall: 0.8624 - val_loss: 1.6258e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.1812e-06 - val_precision: 6.4468e-06 - val_recall: 0.9063\n",
            "Epoch 139/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0010 - mean_io_u: 0.5006 - f1-score: 0.7052 - precision: 0.7149 - recall: 0.8101 - val_loss: 0.0033 - val_mean_io_u: 0.5000 - val_f1-score: 2.1978e-05 - val_precision: 1.1801e-05 - val_recall: 0.9064\n",
            "Epoch 140/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 6.8865e-04 - mean_io_u: 0.4993 - f1-score: 0.7404 - precision: 0.7432 - recall: 0.8366 - val_loss: 1.2266e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.8730e-06 - val_precision: 3.9249e-05 - val_recall: 0.9063\n",
            "Epoch 141/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 5.2908e-04 - mean_io_u: 0.5003 - f1-score: 0.7800 - precision: 0.7793 - recall: 0.8731 - val_loss: 1.5010e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.6148e-05 - val_precision: 3.8880e-05 - val_recall: 0.9063\n",
            "Epoch 142/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.9658e-04 - mean_io_u: 0.5025 - f1-score: 0.7872 - precision: 0.7910 - recall: 0.8761 - val_loss: 1.5418e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.3568e-05 - val_precision: 6.9136e-05 - val_recall: 0.9063\n",
            "Epoch 143/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.7270e-04 - mean_io_u: 0.5034 - f1-score: 0.7926 - precision: 0.7963 - recall: 0.8804 - val_loss: 1.5265e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.7550e-05 - val_precision: 2.3013e-05 - val_recall: 0.9063\n",
            "Epoch 144/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.6456e-04 - mean_io_u: 0.5058 - f1-score: 0.7959 - precision: 0.7979 - recall: 0.8852 - val_loss: 1.6550e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1289e-04 - val_precision: 1.3591e-04 - val_recall: 0.9063\n",
            "Epoch 145/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.6026e-04 - mean_io_u: 0.5061 - f1-score: 0.7964 - precision: 0.7972 - recall: 0.8871 - val_loss: 1.4412e-04 - val_mean_io_u: 0.5000 - val_f1-score: 8.0300e-06 - val_precision: 1.1693e-05 - val_recall: 0.9063\n",
            "Epoch 146/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.5764e-04 - mean_io_u: 0.5074 - f1-score: 0.7987 - precision: 0.7981 - recall: 0.8902 - val_loss: 1.4277e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.5574e-05 - val_precision: 2.2422e-05 - val_recall: 0.9063\n",
            "Epoch 147/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.6677e-04 - mean_io_u: 0.5079 - f1-score: 0.7955 - precision: 0.7990 - recall: 0.8842 - val_loss: 1.4987e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.6575e-05 - val_precision: 2.1316e-05 - val_recall: 0.9063\n",
            "Epoch 148/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.6884e-04 - mean_io_u: 0.5077 - f1-score: 0.7956 - precision: 0.7944 - recall: 0.8885 - val_loss: 1.6391e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.7738e-07 - val_precision: 1.8564e-06 - val_recall: 0.9063\n",
            "Epoch 149/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.8733e-04 - mean_io_u: 0.5066 - f1-score: 0.7896 - precision: 0.7946 - recall: 0.8776 - val_loss: 1.7625e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.2890e-05 - val_precision: 1.1415e-04 - val_recall: 0.9063\n",
            "Epoch 150/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 5.0186e-04 - mean_io_u: 0.5056 - f1-score: 0.7850 - precision: 0.7869 - recall: 0.8797 - val_loss: 1.7672e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0090e-04 - val_precision: 1.1559e-04 - val_recall: 0.9063\n",
            "Epoch 151/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.8522e-04 - mean_io_u: 0.5051 - f1-score: 0.7875 - precision: 0.7927 - recall: 0.8774 - val_loss: 1.4849e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0189e-05 - val_precision: 1.6344e-05 - val_recall: 0.9063\n",
            "Epoch 152/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.0384e-04 - mean_io_u: 0.5048 - f1-score: 0.7829 - precision: 0.7856 - recall: 0.8790 - val_loss: 2.0072e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0218e-04 - val_precision: 1.1524e-04 - val_recall: 0.9063\n",
            "Epoch 153/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.8018e-04 - mean_io_u: 0.5057 - f1-score: 0.7930 - precision: 0.7930 - recall: 0.8856 - val_loss: 2.8114e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.6844e-04 - val_precision: 5.2950e-04 - val_recall: 0.9063\n",
            "Epoch 154/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0012 - mean_io_u: 0.5024 - f1-score: 0.6804 - precision: 0.6978 - recall: 0.7869 - val_loss: 0.0020 - val_mean_io_u: 0.5000 - val_f1-score: 2.4341e-05 - val_precision: 1.3608e-05 - val_recall: 0.9064\n",
            "Epoch 155/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 6.5174e-04 - mean_io_u: 0.4994 - f1-score: 0.7525 - precision: 0.7512 - recall: 0.8498 - val_loss: 1.4521e-04 - val_mean_io_u: 0.5000 - val_f1-score: 7.5420e-06 - val_precision: 1.1716e-05 - val_recall: 0.9063\n",
            "Epoch 156/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.0672e-04 - mean_io_u: 0.5020 - f1-score: 0.7848 - precision: 0.7868 - recall: 0.8746 - val_loss: 1.4720e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.3578e-06 - val_precision: 9.8057e-06 - val_recall: 0.9063\n",
            "Epoch 157/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.5065e-04 - mean_io_u: 0.5051 - f1-score: 0.7983 - precision: 0.7982 - recall: 0.8889 - val_loss: 1.7227e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.1486e-05 - val_precision: 4.8156e-05 - val_recall: 0.9063\n",
            "Epoch 158/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.3076e-04 - mean_io_u: 0.5084 - f1-score: 0.8034 - precision: 0.8038 - recall: 0.8936 - val_loss: 1.7939e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.8554e-05 - val_precision: 5.5228e-05 - val_recall: 0.9063\n",
            "Epoch 159/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.1607e-04 - mean_io_u: 0.5111 - f1-score: 0.8078 - precision: 0.8084 - recall: 0.8974 - val_loss: 1.7363e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.2873e-05 - val_precision: 2.6290e-05 - val_recall: 0.9063\n",
            "Epoch 160/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.1522e-04 - mean_io_u: 0.5134 - f1-score: 0.8061 - precision: 0.8098 - recall: 0.8956 - val_loss: 1.8536e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.6268e-05 - val_precision: 7.4861e-05 - val_recall: 0.9063\n",
            "Epoch 161/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.2268e-04 - mean_io_u: 0.5130 - f1-score: 0.8046 - precision: 0.8065 - recall: 0.8958 - val_loss: 1.8111e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.5066e-05 - val_precision: 6.3434e-05 - val_recall: 0.9063\n",
            "Epoch 162/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.5478e-04 - mean_io_u: 0.5101 - f1-score: 0.7996 - precision: 0.8010 - recall: 0.8910 - val_loss: 1.6717e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.5772e-06 - val_precision: 3.0705e-06 - val_recall: 0.9063\n",
            "Epoch 163/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.3420e-04 - mean_io_u: 0.5083 - f1-score: 0.8028 - precision: 0.8010 - recall: 0.8968 - val_loss: 1.9429e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.1412e-04 - val_precision: 1.2833e-04 - val_recall: 0.9063\n",
            "Epoch 164/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.3609e-04 - mean_io_u: 0.5093 - f1-score: 0.8008 - precision: 0.8049 - recall: 0.8914 - val_loss: 1.7863e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.9305e-05 - val_precision: 6.7572e-05 - val_recall: 0.9063\n",
            "Epoch 165/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.3090e-04 - mean_io_u: 0.5105 - f1-score: 0.8057 - precision: 0.8050 - recall: 0.8986 - val_loss: 1.9777e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.2366e-04 - val_precision: 2.5296e-04 - val_recall: 0.9063\n",
            "Epoch 166/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.3895e-04 - mean_io_u: 0.5107 - f1-score: 0.8037 - precision: 0.8044 - recall: 0.8951 - val_loss: 2.0790e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.7200e-04 - val_precision: 4.1825e-04 - val_recall: 0.9063\n",
            "Epoch 167/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.5163e-04 - mean_io_u: 0.5094 - f1-score: 0.8034 - precision: 0.8025 - recall: 0.8961 - val_loss: 2.0926e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.4892e-04 - val_precision: 2.8271e-04 - val_recall: 0.9063\n",
            "Epoch 168/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.4141e-04 - mean_io_u: 0.5095 - f1-score: 0.8037 - precision: 0.8057 - recall: 0.8942 - val_loss: 1.8453e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0108e-04 - val_precision: 1.1308e-04 - val_recall: 0.9063\n",
            "Epoch 169/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.3851e-04 - mean_io_u: 0.5093 - f1-score: 0.8041 - precision: 0.8056 - recall: 0.8953 - val_loss: 1.8726e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.7251e-04 - val_precision: 1.9555e-04 - val_recall: 0.9063\n",
            "Epoch 170/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.1412e-04 - mean_io_u: 0.5119 - f1-score: 0.8085 - precision: 0.8089 - recall: 0.9004 - val_loss: 2.0384e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9250e-04 - val_precision: 3.2781e-04 - val_recall: 0.9063\n",
            "Epoch 171/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.9874e-04 - mean_io_u: 0.5154 - f1-score: 0.8116 - precision: 0.8152 - recall: 0.9013 - val_loss: 1.9490e-04 - val_mean_io_u: 0.5000 - val_f1-score: 8.6980e-05 - val_precision: 9.7278e-05 - val_recall: 0.9063\n",
            "Epoch 172/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.1622e-04 - mean_io_u: 0.5127 - f1-score: 0.8071 - precision: 0.8085 - recall: 0.8989 - val_loss: 1.8188e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.2685e-05 - val_precision: 1.4132e-05 - val_recall: 0.9063\n",
            "Epoch 173/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.0292e-04 - mean_io_u: 0.5165 - f1-score: 0.8122 - precision: 0.8131 - recall: 0.9029 - val_loss: 2.2627e-04 - val_mean_io_u: 0.5000 - val_f1-score: 4.5315e-04 - val_precision: 5.0344e-04 - val_recall: 0.9063\n",
            "Epoch 174/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.1018e-04 - mean_io_u: 0.5146 - f1-score: 0.8093 - precision: 0.8121 - recall: 0.8992 - val_loss: 2.1334e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9187e-04 - val_precision: 3.2386e-04 - val_recall: 0.9063\n",
            "Epoch 175/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.8228e-04 - mean_io_u: 0.5186 - f1-score: 0.8161 - precision: 0.8180 - recall: 0.9057 - val_loss: 1.9967e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.5717e-05 - val_precision: 6.1892e-05 - val_recall: 0.9063\n",
            "Epoch 176/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 3.7831e-04 - mean_io_u: 0.5210 - f1-score: 0.8181 - precision: 0.8214 - recall: 0.9058 - val_loss: 2.1036e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.4157e-04 - val_precision: 2.6989e-04 - val_recall: 0.9063\n",
            "Epoch 177/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.3724e-04 - mean_io_u: 0.5158 - f1-score: 0.8064 - precision: 0.8105 - recall: 0.8979 - val_loss: 2.4287e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.0928e-06 - val_precision: 1.2807e-05 - val_recall: 0.9063\n",
            "Epoch 178/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.2698e-04 - mean_io_u: 0.5104 - f1-score: 0.8075 - precision: 0.8112 - recall: 0.8959 - val_loss: 2.0051e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.2937e-04 - val_precision: 2.5534e-04 - val_recall: 0.9063\n",
            "Epoch 179/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.2451e-04 - mean_io_u: 0.5121 - f1-score: 0.8075 - precision: 0.8093 - recall: 0.8997 - val_loss: 1.9640e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.8564e-05 - val_precision: 6.5410e-05 - val_recall: 0.9063\n",
            "Epoch 180/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.1224e-04 - mean_io_u: 0.5115 - f1-score: 0.8092 - precision: 0.8118 - recall: 0.9007 - val_loss: 1.9391e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9475e-06 - val_precision: 3.2726e-06 - val_recall: 0.9063\n",
            "Epoch 181/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 3.9048e-04 - mean_io_u: 0.5161 - f1-score: 0.8127 - precision: 0.8171 - recall: 0.9020 - val_loss: 2.1305e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.0386e-05 - val_precision: 6.6870e-05 - val_recall: 0.9063\n",
            "Epoch 182/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.8311e-04 - mean_io_u: 0.5187 - f1-score: 0.8146 - precision: 0.8154 - recall: 0.9078 - val_loss: 2.3333e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9650e-04 - val_precision: 3.2782e-04 - val_recall: 0.9063\n",
            "Epoch 183/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 3.7709e-04 - mean_io_u: 0.5228 - f1-score: 0.8180 - precision: 0.8171 - recall: 0.9110 - val_loss: 2.2499e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.6487e-04 - val_precision: 6.2547e-04 - val_recall: 0.9063\n",
            "Epoch 184/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.0701e-04 - mean_io_u: 0.5180 - f1-score: 0.8125 - precision: 0.8113 - recall: 0.9063 - val_loss: 2.2665e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.9878e-04 - val_precision: 2.2088e-04 - val_recall: 0.9063\n",
            "Epoch 185/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 4.2972e-04 - mean_io_u: 0.5137 - f1-score: 0.8042 - precision: 0.8051 - recall: 0.8983 - val_loss: 2.4365e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.5019e-04 - val_precision: 2.7716e-04 - val_recall: 0.9063\n",
            "Epoch 186/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.4685e-04 - mean_io_u: 0.5138 - f1-score: 0.8050 - precision: 0.8075 - recall: 0.8960 - val_loss: 2.1587e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.5528e-05 - val_precision: 1.0642e-04 - val_recall: 0.9063\n",
            "Epoch 187/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 4.9953e-04 - mean_io_u: 0.5088 - f1-score: 0.7971 - precision: 0.7978 - recall: 0.8914 - val_loss: 2.0229e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0270e-05 - val_precision: 1.3031e-05 - val_recall: 0.9063\n",
            "Epoch 188/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 0.0012 - mean_io_u: 0.4999 - f1-score: 0.6763 - precision: 0.6928 - recall: 0.7787 - val_loss: 1.2912e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.0135e-06 - val_precision: 6.8681e-06 - val_recall: 0.9063\n",
            "Epoch 189/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 5.2172e-04 - mean_io_u: 0.5015 - f1-score: 0.7795 - precision: 0.7825 - recall: 0.8718 - val_loss: 1.5892e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.6733e-05 - val_precision: 2.2311e-05 - val_recall: 0.9063\n",
            "Epoch 190/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 3.9617e-04 - mean_io_u: 0.5103 - f1-score: 0.8115 - precision: 0.8109 - recall: 0.9027 - val_loss: 1.7077e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.0002e-05 - val_precision: 2.4259e-05 - val_recall: 0.9063\n",
            "Epoch 191/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.5583e-04 - mean_io_u: 0.5203 - f1-score: 0.8237 - precision: 0.8241 - recall: 0.9131 - val_loss: 1.7926e-04 - val_mean_io_u: 0.5000 - val_f1-score: 3.0498e-05 - val_precision: 3.4951e-05 - val_recall: 0.9063\n",
            "Epoch 192/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.3190e-04 - mean_io_u: 0.5327 - f1-score: 0.8299 - precision: 0.8300 - recall: 0.9193 - val_loss: 1.7191e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0154e-05 - val_precision: 1.1610e-05 - val_recall: 0.9063\n",
            "Epoch 193/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.1780e-04 - mean_io_u: 0.5417 - f1-score: 0.8340 - precision: 0.8352 - recall: 0.9222 - val_loss: 1.9055e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9156e-05 - val_precision: 3.2615e-05 - val_recall: 0.9063\n",
            "Epoch 194/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.1224e-04 - mean_io_u: 0.5473 - f1-score: 0.8353 - precision: 0.8355 - recall: 0.9246 - val_loss: 2.0124e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.0877e-04 - val_precision: 1.2134e-04 - val_recall: 0.9063\n",
            "Epoch 195/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.1781e-04 - mean_io_u: 0.5510 - f1-score: 0.8341 - precision: 0.8359 - recall: 0.9225 - val_loss: 1.9494e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.9665e-05 - val_precision: 3.3064e-05 - val_recall: 0.9063\n",
            "Epoch 196/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.2929e-04 - mean_io_u: 0.5425 - f1-score: 0.8307 - precision: 0.8307 - recall: 0.9220 - val_loss: 1.9652e-04 - val_mean_io_u: 0.5000 - val_f1-score: 2.7854e-05 - val_precision: 3.0897e-05 - val_recall: 0.9063\n",
            "Epoch 197/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.4488e-04 - mean_io_u: 0.5372 - f1-score: 0.8268 - precision: 0.8287 - recall: 0.9158 - val_loss: 2.0935e-04 - val_mean_io_u: 0.5000 - val_f1-score: 1.4471e-04 - val_precision: 1.6091e-04 - val_recall: 0.9063\n",
            "Epoch 198/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.4099e-04 - mean_io_u: 0.5348 - f1-score: 0.8268 - precision: 0.8283 - recall: 0.9171 - val_loss: 2.0747e-04 - val_mean_io_u: 0.5000 - val_f1-score: 5.0833e-04 - val_precision: 5.6276e-04 - val_recall: 0.9063\n",
            "Epoch 199/200\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 3.4293e-04 - mean_io_u: 0.5331 - f1-score: 0.8286 - precision: 0.8296 - recall: 0.9184 - val_loss: 1.8103e-04 - val_mean_io_u: 0.5000 - val_f1-score: 9.4589e-06 - val_precision: 1.0862e-05 - val_recall: 0.9063\n",
            "Epoch 200/200\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 3.4121e-04 - mean_io_u: 0.5309 - f1-score: 0.8282 - precision: 0.8283 - recall: 0.9191 - val_loss: 2.2861e-04 - val_mean_io_u: 0.5000 - val_f1-score: 6.8919e-04 - val_precision: 7.6894e-04 - val_recall: 0.9063\n"
          ]
        }
      ],
      "source": [
        "#Si uso el custom datagen\n",
        "steps_per_epoch = len(train_img_list)//batch_size\n",
        "val_steps_per_epoch = len(val_img_list)//batch_size\n",
        "\n",
        "history=model.fit(train_img_datagen,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          epochs=200,\n",
        "          verbose=1,\n",
        "          validation_data=val_img_datagen,\n",
        "          validation_steps=val_steps_per_epoch,\n",
        "          )\n",
        "\n",
        "model.save('modelo_1.2.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#plot the training and validation IoU and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "f1 = history.history['f1-score']\n",
        "\n",
        "plt.plot(epochs, f1, 'y', label='Training f1')\n",
        "plt.title('Training and validation f1-score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('f1-score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "JQAT07EfmcZg",
        "outputId": "1899f9a2-c705-434a-b15c-130f6aeeb82c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}